
\documentclass[11pt]{article}

\usepackage{indentfirst}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{natbib}
\usepackage{url}

\newcommand{\set}[1]{\{\, #1 \,\}}

\newcommand{\y}{\texttt{y}}
\newcommand{\x}[1]{\texttt{x#1}}

\newcommand{\opand}{\mathbin{\rm and}}

\let\emptyset=\varnothing
\let\code=\texttt

\newtheorem{theorem}{Theorem}

%\VignetteEngine{knitr::knitr}

\begin{document}

\title{Limiting Conditional Models for Categorical Data Analysis}

\author{Charles J. Geyer}

\maketitle

<<options,include=FALSE,echo=FALSE>>=
options(keep.source = TRUE, width = 60)
@

This document applies the theory of \citet{gdor} to categorical data analysis.
Also relevant are the lecture notes for Stat 5421 \citep{expfam,infinity}.

We say we are doing categorical data analysis if
\begin{enumerate}
\item the response vector is a vector of counts (nonnegative-integer-valued),
      \label{item:response-categorical}
\item the sampling distribution is Poisson, multinomial, or product multinomial
    \citep[Section~7]{expfam}, and
      \label{item:sampling}
\item all predictor variables are categorical, hence all regressor variables
    (columns of the model matrix) are zero-or-one-valued 
    \citep[Section~4.6]{expfam}.
      \label{item:predictor-categorical}
\end{enumerate}
Items \ref{item:response-categorical}
and \ref{item:predictor-categorical}
are basic to what is meant by categorical data analysis: both response and
predictor variables are categorical.
Item \ref{item:sampling} codifies the standard assumption for the distribution
of count data.

From Section~3.9 of \citet{gdor} we see that if $y$ is the response vector
and $M$ is the model matrix, then $M^T y$ is the submodel canonical sufficient
statistic vector.  Theorems 1, 3, and {4} in \citet{gdor} say that the
maximum likelihood estimate (MLE) exists for the submodel canonical parameter
vector (called the coefficients vector by R function \code{glm})
if and only if every direction of recession of the log likelihood is also
a direction of constancy, where if $Y$ denotes a random realization of the
response vector and $y$ the observed value of the response vector, a vector
$\delta$ in the submodel canonical parameter space is
\begin{itemize}
\item a \emph{direction of recession} if $(Y - y)^T M \delta \le 0$
    almost surely and
\item and a \emph{direction of constancy} if $(Y - y)^T M \delta = 0$
    almost surely.
\end{itemize}

We now assume Poisson sampling, relying on the theorems in Section~7
of \citet{expfam} that say we get the same results for the other sampling
models.

Suppose $\delta$ is a nonzero direction of recession
and define $\eta = M \delta$.  Then by definition of direction of recession,
we have $Y^T \eta \le y^T \eta$ almost surely.
\begin{itemize}
\item If $\eta_j = 0$, this says nothing about $Y_j$.
\item If $\eta_j < 0$, this says $Y_j \ge y_j$ almost surely because it
    is possible that all coordinates of $Y$ are zero except for $Y_j$.
    And this implies $y_j = 0$ because it it possible for $Y_j$ to be zero.
\item If $\eta_j > 0$, this says $Y_j \le y_j$ almost surely, and this is
    impossible because $Y_j$ can take arbitrarily large values.  Thus this
    case in not allowed.
\end{itemize}
Thus for any direction of recession $\eta$ we must have all coordinates
nonpositive, and we must have $\eta_j < 0$ implies $y_j = 0$.

We can search for such vectors with linear programming.  Consider the
following linear programming problem.  Let $I = \{1, \ldots, k\}$ be the
index set of the vectors $y$ and $\eta$ and the row index set of the model
matrix $M$, and $J = \{1, \ldots, p\}$ be the
index set of the vector $\delta$ and the column index set of the model
matrix $M$, and let $m_{i j}$ denote the components of the model matrix.
\begin{alignat*}{2}
  \text{minimize}   & \ \sum_{\substack{i \in I \\ y_i = 0}} \sum_{j \in J}
      m_{i j} \delta_j
  \\
  \text{subject to} & \ \sum_{j \in J} m_{i j} \delta_j = 0,
      & \qquad & i \in I \opand y_i > 0
  \\
                    & \ -1 \le \sum_{j \in J} m_{i j} \delta_j \le 0,
      & & i \in I \opand y_i = 0
\end{alignat*}
In fact we can do better than this if we have a fitted data vector $\hat{mu}$
from an attempt by R function \code{glm} (or some other likelihood maximizer)
to maximize the likelihood.  From Theorem~{6} in \citet{gdor} we must have
$(Y - y)^T M \delta = 0$ almost surely for the limiting conditional model.
Hence we must have $(\mu - y)^T M \delta = 0$ almost surely.  Hence
(still using $\eta = M \delta$) we must have $\eta_j < 0$ implies $\mu_j = 0$
and, conversely, $\mu_j > 0$ implies $\eta_j = 0$.

Due to inexactness of computer arithmetic and due to the approximateness
of computer optimization routines, we can never tell whether what the computer
reports as fitted values are zero or not.  We have to use a tolerance.
To respect this issue we adopt the notation $\hat{\mu}_j \gg 0$ indicates
that $\hat{\mu}_j$ is larger than the tolerance and $\hat{\mu}_j \simeq 0$
indicates that $\hat{\mu}_j$ is not larger than the tolerance.
Then we can use this linear program
\begin{alignat*}{2}
  \text{minimize}   & \ \sum_{\substack{i \in I \\ \hat{\mu}_i \simeq 0}}
      \sum_{j \in J} m_{i j} \delta_j
  \\
  \text{subject to} & \ \sum_{j \in J} m_{i j} \delta_j = 0,
      & \qquad & i \in I \opand \hat{\mu}_i \gg 0
  \\
                    & \ -1 \le \sum_{j \in J} m_{i j} \delta_j \le 0,
      & & i \in I \opand \hat{\mu}_i \simeq 0
\end{alignat*}

The fact that our tolerance may be wrong and we may be misclassifying
some of the components of $\hat{\mu}$ as zero or nonzero does no harm
so long as the tolerance is large enough.  If we decide $\hat{\mu}_j \simeq 0$
when in fact $\mu_j > 0$, then our linear program just has to do more work
but is not wrong.  Any solution is still a direction of recession.
In fact, no matter what we can still easily check whether a solution of
our linear program is a direction of recession (just check that $\eta_j \le 0$
for all $j$ and $\eta_j < 0$ implies $y_j = 0$).

In this we have come close to re-inventing "Algorithm R" of
\citet[Section~2.4]{thesis}.  The only difference is that "Algorithm R" is
not just for Poisson GLM and it also considers approximate problems using
only a subset of the constraints, which was necessary for the application
that was the main motivation \citep[Chapter~6]{thesis}.

Note that with using $\hat{\mu}$ to determine the linear programming problem,
there is often nothing to do.  If we have $\hat{\mu}_i \gg 0$ for all $i$ then
the only directions of recession are vectors in the null space of $M$ and
those are also directions of constancy, so the MLE exists in the classical
sense.

It is only when $\hat{\mu}_j \simeq 0$ for some $j$ (and typically R function
\code{glm} issues a warning) that we have to follow up with doing a linear
program.

So let us try this out.  We will use the data for the example
of Section~{2.3} in \citet{gdor}.
<<data, error=TRUE>>=
u <- "http://www.stat.umn.edu/geyer/gdor/catrec.txt"
dat <- read.table(u, header = TRUE)
gout <- glm(y ~ (v1 + v2 + v3 + v4 + v5 + v6 + v7)^3,
    family = poisson, data = dat, x = TRUE)
@
We see that R function \code{glm} does not warn about these data and this
model, even though we know (from the analysis in the cited article) that
there is a direction of recession and the MLE for the submodel canonical
parameter (what R function \code{glm} calls coefficients) does not exist.

\begin{itemize}
\item The version of R used to make this document is \Sexpr{getRversion()}.
\item The version of the \texttt{knitr} package used to make this document is
    \Sexpr{packageVersion("knitr")}.
\item The version of the \texttt{rcdd} package used to make this document is
    \Sexpr{packageVersion("rcdd")}.
\end{itemize}
<<library>>=
library(rcdd)
@

So let us try our new algorithm.
<<new-algorithm-setup>>=
m <- gout$x
mu.hat <- gout$fitted
min(mu.hat)
sort(mu.hat)[sort(mu.hat) < 1000 * min(mu.hat)]
@
We seem to get just these 16 values as approximate zeros of $\hat{\mu}$
no matter what reasonable tolerance we choose.  Clearly R function \code{glm}
uses a tolerance too small for these data and this model, because it fails
to warn when it should.
<<new-algorithm-setup-too>>=
is.zero <- mu.hat < 1000 * min(mu.hat)
sum(is.zero)
sum(gout$y[is.zero])
@

We will use R function \code{lpcdd} in R package \code{rcdd} to do the
linear programming.  Its help file says
\begin{quotation}
This function minimizes or maximizes an affine function \code{x}
maps to \code{sum(objgrd * x) + objcon} over a convex polyhedron
given by the H-representation given by the matrix \code{hrep}.
Let
\begin{verbatim}
      l <- hrep[ , 1]
      b <- hrep[ , 2]
      v <- hrep[ , - c(1, 2)]
      a <- (- v)
\end{verbatim}

Then the convex polyhedron in question is the set of
points \code{x} satisfying
\begin{verbatim}
      axb <- a \%*\% x - b
      all(axb <= 0)
      all(l * axb == 0)
\end{verbatim}
\end{quotation}

So we need
<<new-algorithm-setup-too-too>>=
objgrd <- as.numeric(rbind(as.numeric(is.zero)) %*% m)
@

Now our constraints are of the form $A \delta \le b$ for some matrix $A$
and some right-hand-side vector $b$ with $l * A \delta = l * b$ for some
zero-or-one-valued vector $l$, where $*$ denotes componentwise multiplication
like R does.
<<new-algorithm-setup-too-too-too>>=
a1 <- m[! is.zero, ]
b1 <- rep(0, nrow(a1))
l1 <- rep(1, nrow(a1))
a2 <- m[is.zero, ]
b2 <- rep(0, nrow(a2))
l2 <- rep(0, nrow(a2))
a3 <- - m[is.zero, ]
b3 <- rep(1, nrow(a3))
l3 <- rep(0, nrow(a3))
a <- rbind(a1, a2, a3)
b <- c(b1, b2, b3)
l <- c(l1, l2, l3)
hrep <- cbind(l, b, - a)
# convert to infinite-precision rational
stopifnot(objgrd == round(objgrd))
objgrd <- d2q(round(objgrd))
stopifnot(hrep == round(hrep))
hrep <- d2q(round(hrep))
is.matrix(hrep)
@

Now we are ready for linear programming.
<<new-algorithm>>=
lout <- lpcdd(hrep, objgrd)
names(lout)
lout$solution.type
lout$optimal.value
delta <- q2d(lout$primal.solution)
eta <- as.numeric(m %*% delta)
all(eta <= 0)
sum(eta < 0)
identical(eta < 0, as.vector(is.zero))
@

Great!  We found the same LCM as in the cited article.  We may have
a different direction of recession though.
<<print-dor>>=
names(delta) <- names(gout$coefficients)
cbind(delta)[delta != 0, , drop = FALSE]
@
Completely agrees with the GDOR in the cited article.  Oh!  Now I remember.
It has to agree up to multiplication by a positive scalar, because the
dimension of the LCM happens to be only one less than the original model.

Let's try fitting the LCM.
<<fit-lcm>>=
gout.lcm <- glm(y ~ (v1 + v2 + v3 + v4 + v5 + v6 + v7)^3,
    family = poisson, data = dat, x = TRUE, subset = eta == 0)
sum(is.na(gout.lcm$coefficients))
@

Compare AIC and BIC.
<<aic>>=
AIC(gout)
AIC(gout.lcm)
foo <- logLik(gout)
all.equal(- 2 * as.numeric(foo) + 2 * attr(foo, "df"),
    AIC(gout))
foo <- logLik(gout.lcm)
all.equal(- 2 * as.numeric(foo) + 2 * attr(foo, "df"),
    AIC(gout.lcm))
@
The models differ by one degree of freedom but have the same maximized
value of the log likelihood.  So this makes sense.
<<bic>>=
BIC(gout)
BIC(gout.lcm)
foo <- logLik(gout)
- 2 * as.numeric(foo) + log(sum(gout$y)) * attr(foo, "df")
foo <- logLik(gout.lcm)
- 2 * as.numeric(foo) + log(sum(gout.lcm$y)) * attr(foo, "df")
@
R function \code{BIC} gives complete nonsense because it considers
the degrees of freedom to be the number of cells in the contingency table
rather than the number of individuals classified.  This is clearly wrong
because when we are thinking about contingency tables $n$ is the sample size
\citep{kass-raftery,raftery}.
Our calculations here are correct.



\begin{thebibliography}{}

\bibitem[Geyer(1990)]{thesis}
\newblock Likelihood and Exponential Families.
\newblock PhD thesis, University of Washington.
\newblock \url{http://hdl.handle.net/11299/56330}.

\bibitem[Geyer(2009)]{gdor}
Geyer, C.~J. (2009).
\newblock Likelihood inference in exponential families and directions of
    recession.
\newblock \emph{Electronic Journal of Statistics}, \textbf{3}, 259--289
    (electronic).
\newblock \url{http:www.stat.umn.edu/geyer/gdor/}.

\bibitem[Geyer(2016a)]{expfam}
Geyer, C.~J. (2016a).
\newblock Stat 5421 Lecture Notes: Exponential Families, Part I.
\newblock \url{http://www.stat.umn.edu/geyer/5421/notes/expfam.pdf}.

\bibitem[Geyer(2016b)]{infinity}
Geyer, C.~J. (2016b).
\newblock Stat 5421 Lecture Notes: Exponential Families, Part II.
\newblock \url{http://www.stat.umn.edu/geyer/5421/notes/infinity.pdf}.

\bibitem[Kass and Raftery(1995)]{kass-raftery}
Kass, R.~E., and Raftery, A.~E. (1995).
\newblock Bayes factors.
\newblock \emph{Journal of the American Statistical Association},
    \textbf{90}, 773--795.

\bibitem[Raftery(1986)]{raftery}
Raftery, A.~E. (1986).
\newblock A note on Bayes factors for log‐linear contingency table models
    with vague prior information.
\newblock \emph{Journal of the Royal Statistical Society, Series B},
    \textbf{48}, 249--250.

\end{thebibliography}

\end{document}

