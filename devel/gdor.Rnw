
\documentclass[11pt]{article}

\usepackage{indentfirst}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{natbib}
\usepackage{url}

\newcommand{\set}[1]{\{\, #1 \,\}}

\newcommand{\y}{\texttt{y}}
\newcommand{\x}[1]{\texttt{x#1}}

\newcommand{\opand}{\mathbin{\rm and}}

\let\emptyset=\varnothing
\let\code=\texttt

\newtheorem{theorem}{Theorem}

\newcommand{\REVISED}{\begin{center} \LARGE REVISED DOWN TO HERE \end{center}}
\newcommand{\MOVED}[1][equation]{\begin{center} [#1 moved] \end{center}}

%\VignetteEngine{knitr::knitr}

\begin{document}

\title{Limiting Conditional Models for Categorical Data Analysis}

\author{Charles J. Geyer}

\maketitle

<<options,include=FALSE,echo=FALSE>>=
options(keep.source = TRUE, width = 60)
@

\section{R}

\begin{itemize}
\item The version of R used to make this document is \Sexpr{getRversion()}.
\item The version of R package \code{knitr} used to make this document is
   \Sexpr{packageVersion("knitr")}.
\item The version of R package \code{biglm} used to make this document is
   \Sexpr{packageVersion("biglm")}.
\item The version of R package \code{clpAPI} used to make this document is
   \Sexpr{packageVersion("clpAPI")}.
\item The version of R package \code{Matrix} used to make this document is
   \Sexpr{packageVersion("Matrix")}.
\end{itemize}

\section{Theory}

This document applies the theory of \citet{gdor} to categorical data analysis.
Also relevant are the lecture notes for Stat 5421 \citep{expfam,infinity}.

We say we are doing categorical data analysis if
\begin{itemize}
\item the response vector is a vector of counts (nonnegative-integer-valued) and
\item the sampling distribution is Poisson, multinomial (including binomial),
      or product multinomial \citep[Section~7]{expfam}.
\end{itemize}

From Section~3.9 of \citet{gdor} we see that if $y$ is the response vector
and $M$ is the model matrix, then $M^T y$ is the submodel canonical sufficient
statistic vector.  Theorems 1, 3, and {4} in \citet{gdor} say that the
maximum likelihood estimate (MLE) exists for the submodel canonical parameter
vector (called the coefficients vector by R function \code{glm})
if and only if every direction of recession of the log likelihood is also
a direction of constancy, where, if $Y$ denotes a random realization of the
response vector and $y$ the observed value of the response vector, a vector
$\delta$ in the submodel canonical parameter space is
\begin{itemize}
\item a \emph{direction of recession} (DOR) if $(Y - y)^T M \delta \le 0$
    almost surely and
\item and a \emph{direction of constancy} (DOC) if $(Y - y)^T M \delta = 0$
    almost surely.
\end{itemize}

We now assume Poisson sampling, relying on the theorems in Section~7
of \citet{expfam} that say we get the same results for the other sampling
models.

Suppose $\delta$ is a nonzero direction of recession
and define $\eta = M \delta$.  Then by definition of direction of recession,
we have $Y^T \eta \le y^T \eta$ almost surely.
\begin{itemize}
\item If $\eta_j = 0$, this says nothing about $Y_j$.
\item If $\eta_j < 0$, this says $Y_j \ge y_j$ almost surely because it
    is possible that all coordinates of $Y$ are zero except for $Y_j$.
    And this implies $y_j = 0$ because it it possible for $Y_j$ to be zero.
\item If $\eta_j > 0$, this says $Y_j \le y_j$ almost surely because it
    is possible that all coordinates of $Y$ are zero except for $Y_j$.
    And this is impossible because $Y_j$ can take arbitrarily large values.
    Thus this case in not allowed.
\end{itemize}
Thus for any direction of recession $\eta$ we must have all coordinates
nonpositive, and we must have $\eta_j < 0$ implies $y_j = 0$.

We can search for such vectors with linear programming.  Consider the
following linear programming problem.  Let $I = \{1, \ldots, k\}$ be the
index set of the vectors $y$ and $\eta$ and the row index set of the model
matrix $M$, and $J = \{1, \ldots, p\}$ be the
index set of the vector $\delta$ and the column index set of the model
matrix $M$, and let $m_{i j}$ denote the components of the model matrix.
\begin{alignat}{2}
  \text{minimize}   & \ \sum_{\substack{i \in I \\ y_i = 0}} \sum_{j \in J}
      m_{i j} \delta_j
  \nonumber
  \\
  \text{subject to} & \ \sum_{j \in J} m_{i j} \delta_j = 0,
      & \qquad & i \in I \opand y_i > 0
  \label{prog:no-mu-hat}
  \\
                    & \ -1 \le \sum_{j \in J} m_{i j} \delta_j \le 0,
      & & i \in I \opand y_i = 0
  \nonumber
\end{alignat}

\begin{theorem}
An MLE exists in the conventional sense (for the canonical parameter)
if and only if the linear program \eqref{prog:no-mu-hat} has optimal value
zero.  When the optimal value is negative, the solution $\delta$ is a
direction of recession that is not a direction of constancy and taking
limits in that direction gives a limiting conditional model having smaller
support than the original model.
\end{theorem}
See Theorem~6 and the following discussion in \citet{gdor} for discussion
of limits in directions of recession and the resulting limiting conditional
models.
\begin{proof}
As the discussion preceeding the theorem says, we have a DOR
if and only if $\eta_j \le 0$ for all $j$ and $\eta_j < 0$ implies $y_j = 0$.
Redoing that discussion with DOC instead of DOR, we have a DOC
if and only if $\eta_j = 0$ for all $j$, that is, if $\delta$ is in the
null space of the model matrix.
By Theorem~{4} in \citet{gdor} the MLE exists in the conventional sense
if and only if every DOR is a DOC.  But since the optimal value of the
linear program is the sum of the components of $\eta$, this optimal value
is zero if and only if there is no $\delta$ such that any component of
$\eta$ is nonzero, that is if there is no DOR that is not also a DOC.

Conversely, if the optimal value is nonzero, then there is a delta that
is a DOR and the rest follows from Theorem~{6} in \citet{gdor} and the
discussion following that theorem in that article.
\end{proof}

In fact we can do better than this if we have a fitted data vector $\hat{\mu}$
from an attempt by R function \code{glm} (or some other likelihood maximizer)
to maximize the likelihood.  From Theorem~{6} in \citet{gdor} we must have
$(Y - y)^T M \delta = 0$ almost surely for the limiting conditional model.
Hence we must have $(\mu - y)^T M \delta = 0$ almost surely.  Hence
(still using $\eta = M \delta$) we must have $\eta_j < 0$ implies $\mu_j = 0$
and, conversely, $\mu_j > 0$ implies $\eta_j = 0$.

Due to inexactness of computer arithmetic and due to the approximateness
of computer optimization routines, we can never tell whether what the computer
reports as fitted values are zero or not.  We have to use a tolerance.
To respect this issue we adopt the notation $\hat{\mu}_j \gg 0$ indicates
that $\hat{\mu}_j$ is larger than the tolerance and $\hat{\mu}_j \simeq 0$
indicates that $\hat{\mu}_j$ is not larger than the tolerance.
Then we can use this linear program
\begin{alignat}{2}
  \text{minimize}   & \ \sum_{\substack{i \in I \\ \hat{\mu}_i \simeq 0}}
      \sum_{j \in J} m_{i j} \delta_j
  \nonumber
  \\
  \text{subject to} & \ \sum_{j \in J} m_{i j} \delta_j = 0,
      & \qquad & i \in I \opand \hat{\mu}_i \gg 0
  \label{prog:mu-hat}
  \\
                    & \ -1 \le \sum_{j \in J} m_{i j} \delta_j \le 0,
      & & i \in I \opand \hat{\mu}_i \simeq 0
  \nonumber
\end{alignat}

The fact that our tolerance may be wrong and we may be misclassifying
some of the components of $\hat{\mu}$ as zero or nonzero does no harm
so long as the tolerance is large enough.  If we decide $\hat{\mu}_j \simeq 0$
when in fact $\mu_j > 0$, then our linear program just has to do more work
but is not wrong.  Any solution is still a direction of recession.
In fact, no matter what, we can still easily check whether a solution of
our linear program is a direction of recession (just check that $\eta_j \le 0$
for all $j$ and $\eta_j < 0$ implies $y_j = 0$).

Note that with using $\hat{\mu}$ to determine the linear programming problem,
there is often nothing to do.  If we have $\hat{\mu}_i \gg 0$ for all $i$ then
the only directions of recession are vectors in the null space of $M$ and
those are also directions of constancy, so the MLE exists in the classical
sense.

It is only when $\hat{\mu}_j \simeq 0$ for some $j$ (and typically R function
\code{glm} issues a warning) that we have to follow up with doing a linear
program.

Although this linear program is guaranteed to find a DOR that is not a DOC
if one exists, it is not guaranteed to find a \emph{generic} DOR \citep{gdor}.
Thus this algorithm may need to be iterated.  The limiting conditional model
it finds may itself have a DOR that is not a DOC, so we may need to apply
the algorithm to it.

\section{Practice}

\subsection{R Packages}

<<packages>>=
library(biglm)
library(clpAPI)
library(Matrix)
@

\subsection{Data}

We do an example taking data from a draft paper \citep{eck-geyer}
that is too big to do using the methods of \citet{gdor} in a reasonable
amount of time (they take several days of computing time).
<<read.big.data>>=
foo <- paste0("https://conservancy.umn.edu/bitstream/handle/",
    "11299/197369/bigcategorical.txt")
bar <- sub("^.*/", "", foo)
if (! file.exists(bar)) download.file(foo, bar)
bigcategorical <- read.table(bar, header = TRUE,
    stringsAsFactors = TRUE)
class(bigcategorical)
dim(bigcategorical)
names(bigcategorical)
@

\subsection{Fit GLM}

Fit model specified in the paper
(all 4 way and no 5 way interactions).
<<big.data.fit,error=TRUE,cache=TRUE>>=
gout <- bigglm(y ~ 0 + (x1 + x2 + x3 + x4 + x5)^4,
    family = poisson(), data = bigcategorical, maxit = 200)
gout$converged
cout <- coef(gout)
length(cout)
! any(is.na(cout))
@
The last statement says that, as far as R function \code{bigglm} can tell,
there are no DOC in this model.

\subsection{Mean-Value Parameters}

Now get mean-value parameters
<<big.data.mean.value,error=TRUE,cache=TRUE>>=
mu <- predict(gout, newdata = bigcategorical,
    type = "response")
length(mu)
@

Now we check what are small and large values of components of $\mu$.
<<big.data.mean.value.sizes>>=
foo <- log10(sort(mu))
dfoo <- diff(foo)
i <- which(dfoo == max(dfoo))
foo[seq(max(1, i - 2), min(length(foo), i + 2))]
@
Looks like we have a jump in $\mu$ components from roughly
$10^{\Sexpr{round(foo[i])}}$ to $10^{\Sexpr{round(foo[i + 1])}}$.
Anywhere in there will do as a cutoff.
<<big.data.mean.value.is.zero>>=
is.zero <- mu < sqrt(.Machine$double.eps)
@

\subsection{Model Matrix}

R function \code{bigglm} does not use an explicit model matrix.
We need the model matrix to construct the linear program.
In order to not blow out memory, we use R function \code{sparse.model.matrix}
from R package \code{Matrix}.
<<sparse.model.matrix.bigcategorical>>=
m <- sparse.model.matrix(
    y ~ 0 + (x1 + x2 + x3 + x4 + x5)^4,
    data = bigcategorical)
dim(m)
@

\subsection{Linear Program Specification}

\subsubsection{Objective Function}

Objective function gradient
<<objgrd.bigcategorical>>=
objgrd <- rbind(as.numeric(is.zero)) %*% m
objgrd <- as(objgrd, "numeric")
@

\subsubsection{Initialize LP Object}

<<clpAPI.initialize.bigcategorical>>=
lp <- initProbCLP()
@

Set row number
<<clpAPI.resize.bigcategorical>>=
resizeCLP(lp, nrow(m), 0)
@

And also this, to shut up printout to standard error.
<<clpAPI.blather.bigcategorical>>=
setLogLevelCLP(lp, 0)
@

\subsubsection{Put in LP Data}

Add columns
<<clpAPI.add.columns.bigcategorical>>=
foo <- rep(Inf, ncol(m))
addColsCLP(lp, ncol(m), -foo, foo, objgrd, m@p, m@i, m@x)
@
Set the row bounds
<<clpAPI.row.bounds.bigcategorical>>=
chgRowLowerCLP(lp, -as.numeric(is.zero))
chgRowUpperCLP(lp, rep(0, nrow(m)))
@
Set optimization "direction" to minimize.
<<clpAPI.minimize.bigcategorical>>=
setObjDirCLP(lp, 1)
@

\subsection{Solve LP}

R package \code{clpAPI} has five functions to solve linear programs.
Not knowing which one to use, we try one.
<<clpAPI.solve.bigcategorical>>=
lptime <- system.time(primalCLP(lp))
getSolStatusCLP(lp)
@
Solution status = 0 means optimal.
<<solution.time>>=
lptime
@

The solution is a generalized direction of recession (GDOR)
<<clpAPI.delta.bigcategorical>>=
delta <- getColPrimCLP(lp)
@

\subsection{Check Solution}

Check that we have solution.  First map GDOR to saturated model
canonical parameter scale.
<<eta.bigcategorical>>=
eta <- m %*% delta
eta <- as(eta, "numeric")
@

Now check all components of $\eta$ nonnegative
<<eta.check.one.bigcategorical>>=
max(eta)
@
Inaccuracy of computer arithmetic strikes again.  I guess that's OK.

Now check that $\eta_i < 0$ implies $y_i = 0$.
Actually check the contrapositive: $y_i > 0$ implies $\eta_i = 0$.
<<eta.check.two.bigcategorical>>=
range(eta[bigcategorical$y > 0])
@
Looks OK.

\subsection{Linearity}

Now we check what \citet{gdor} calls the linearity following R package
\code{rcdd}.  What is the dimension of the linearity?
<<linearity.bigcategorical>>=
sum(eta >= (- sqrt(.Machine$double.eps)))
sum(eta < (- sqrt(.Machine$double.eps)))
@
This does not agree with the supplementary material for
\citet{eck-geyer}.  Looks like we have a DOR but not a GDOR.  
Thus we must iterate the process.

\section{Practice: Second Iteration}

\subsection{Save GDOR}

<<lcm.big.cat.save.gdor>>=
save.delta <- delta
@

\subsection{Fit GLM for LCM}

Limiting conditional model (LCM) conditions on the event
$\langle y, \eta \rangle = 0$, that is, on $\eta_j < 0$ implies $Y_j = 0$.
To fit this model, we subset the data
<<lcm.big.cat.subset>>=
is.zero.lcm <- eta < (- sqrt(.Machine$double.eps))
bigcategorical.sub <- subset(bigcategorical,
    subset = ! is.zero.lcm)
dim(bigcategorical.sub)
@

Fit LCM
<<big.data.fit.lcm,error=TRUE,cache=TRUE>>=
gout <- bigglm(y ~ 0 + (x1 + x2 + x3 + x4 + x5)^4,
    family = poisson(), data = bigcategorical.sub,
    maxit = 200)
@
Well, that's a shock.  Unlike R function \code{glm}, R function \code{bigglm}
cannot handle collinearity.  We will just have to do without it.

\subsection{Zero Data Values}

<<big.data.mean.value.is.zero.lcm>>=
is.zero <- bigcategorical.sub$y == 0
@

\subsection{Model Matrix}

<<sparse.model.matrix.bigcategorical.lcm>>=
m <- sparse.model.matrix(
    y ~ 0 + (x1 + x2 + x3 + x4 + x5)^4,
    data = bigcategorical.sub)
dim(m)
@

\subsection{Linear Program Specification}

<<linprog.lcm>>=
# objective function gradient
objgrd <- rbind(as.numeric(is.zero)) %*% m
objgrd <- as(objgrd, "numeric")
# destruct LP object
delProbCLP(lp)
# initialize LP object
lp <- initProbCLP()
# set row number
resizeCLP(lp, nrow(m), 0)
# shut up printout to standard error.
setLogLevelCLP(lp, 0)
# add columns
foo <- rep(Inf, ncol(m))
addColsCLP(lp, ncol(m), -foo, foo, objgrd, m@p, m@i, m@x)
# set the row bounds
chgRowLowerCLP(lp, -as.numeric(is.zero))
chgRowUpperCLP(lp, rep(0, nrow(m)))
# set optimization direction to minimize
setObjDirCLP(lp, 1)
# solve LP
lptime <- system.time(primalCLP(lp))
getSolStatusCLP(lp) == 0
lptime
delta <- getColPrimCLP(lp)
@

\subsection{Check Solution}

<<eta.bigcategorical.lcm>>=
eta <- m %*% delta
eta <- as(eta, "numeric")
# check all components of eta nonnegative
max(eta)
# check that y[j] > 0 implies eta[j] == 0
range(eta[bigcategorical.sub$y > 0])
@

\subsection{Linearity}

Now we have a problem matching up the LCM with the original model.
<<lcm.and.om>>=
idx <- as.numeric(rownames(bigcategorical.sub))
length(idx)
length(eta)
eta.om <- double(nrow(bigcategorical))
eta.om[idx] <- eta
sum(is.zero.lcm)
length(eta.om)
length(is.zero.lcm)
is.zero.lcm <- is.zero.lcm | eta.om < (- sqrt(.Machine$double.eps))
sum(is.zero.lcm)

##### ?????
range(bigcategorical$y[is.zero.lcm])
@

\begin{thebibliography}{}

\bibitem[Eck and Geyer(2018)]{eck-geyer}
Eck, D.~J., and Geyer, C.~J. (2018).
\newblock Computationally efficient likelihood inference in exponential
    families when the maximum likelihood estimator does not exist.
\newblock \url{https://arxiv.org/abs/1803.11240}.

\bibitem[Geyer(1990)]{thesis}
Geyer, C.~J. (1990).
\newblock Likelihood and Exponential Families.
\newblock PhD thesis, University of Washington.
\newblock \url{http://hdl.handle.net/11299/56330}.

\bibitem[Geyer(2009)]{gdor}
Geyer, C.~J. (2009).
\newblock Likelihood inference in exponential families and directions of
    recession.
\newblock \emph{Electronic Journal of Statistics}, \textbf{3}, 259--289
    (electronic).
\newblock \url{http:www.stat.umn.edu/geyer/gdor/}.

\bibitem[Geyer(2016a)]{expfam}
Geyer, C.~J. (2016a).
\newblock Stat 5421 Lecture Notes: Exponential Families, Part I.
\newblock \url{http://www.stat.umn.edu/geyer/5421/notes/expfam.pdf}.

\bibitem[Geyer(2016b)]{infinity}
Geyer, C.~J. (2016b).
\newblock Stat 5421 Lecture Notes: Exponential Families, Part II.
\newblock \url{http://www.stat.umn.edu/geyer/5421/notes/infinity.pdf}.

\end{thebibliography}

\end{document}

