
\documentclass[11pt]{article}

\usepackage{indentfirst}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{natbib}
\usepackage{url}

\newcommand{\set}[1]{\{\, #1 \,\}}

\newcommand{\opand}{\mathbin{\rm and}}

\let\code=\texttt

\newtheorem{theorem}{Theorem}

\newcommand{\REVISED}{\begin{center} \LARGE REVISED DOWN TO HERE \end{center}}
\newcommand{\MOVED}[1][equation]{\begin{center} [#1 moved] \end{center}}

%\VignetteEngine{knitr::knitr}

\begin{document}

\title{Limiting Conditional Models for Categorical Data Analysis}

\author{Charles J. Geyer}

\maketitle

<<options,include=FALSE,echo=FALSE>>=
options(keep.source = TRUE, width = 60)
@

\section{R}

\begin{itemize}
\item The version of R used to make this document is \Sexpr{getRversion()}.
\item The version of R package \code{knitr} used to make this document is
   \Sexpr{packageVersion("knitr")}.
\item The version of R package \code{clpAPI} used to make this document is
   \Sexpr{packageVersion("clpAPI")}.
\item The version of R package \code{Matrix} used to make this document is
   \Sexpr{packageVersion("Matrix")}.
\end{itemize}

\section{Theory}

This document applies the theory of \citet{gdor} to categorical data analysis.
Also relevant are the lecture notes for Stat 5421 \citep{expfam,infinity}.

We say we are doing categorical data analysis if
\begin{itemize}
\item the response vector is a vector of counts (nonnegative-integer-valued),
\item the sampling distribution is Poisson, multinomial (including binomial),
      or product multinomial \citep[Section~7]{expfam}, and
\item we use canonical link function that makes our model a regular full
      exponential family.
\end{itemize}

From Section~3.9 of \citet{gdor} we see that if $y$ is the response vector
and $M$ is the model matrix, then $M^T y$ is the submodel canonical sufficient
statistic vector.  Theorems 1, 3, and {4} in \citet{gdor} say that the
maximum likelihood estimate (MLE) exists for the submodel canonical parameter
vector (called the coefficients vector by R function \code{glm})
if and only if every direction of recession of the log likelihood is also
a direction of constancy, where, if $Y$ denotes a random realization of the
response vector and $y$ the observed value of the response vector, a vector
$\delta$ in the submodel canonical parameter space is
\begin{itemize}
\item a \emph{direction of recession} (DOR) if $(Y - y)^T M \delta \le 0$
    almost surely and
\item and a \emph{direction of constancy} (DOC) if $(Y - y)^T M \delta = 0$
    almost surely.
\end{itemize}

We now assume Poisson sampling, relying on the theorems in Section~7
of \citet{expfam} that say we get the same results for the other sampling
models.

Suppose $\delta$ is a nonzero direction of recession
and define $\eta = M \delta$.  Then by definition of direction of recession,
we have $Y^T \eta \le y^T \eta$ almost surely.
\begin{itemize}
\item If $\eta_j = 0$, this says nothing about $Y_j$.
\item If $\eta_j < 0$, this says $Y_j \ge y_j$ almost surely because it
    is possible that all coordinates of $Y$ are zero except for $Y_j$.
    And this implies $y_j = 0$ because it it possible for $Y_j$ to be zero.
\item If $\eta_j > 0$, this says $Y_j \le y_j$ almost surely because it
    is possible that all coordinates of $Y$ are zero except for $Y_j$.
    And this is impossible because $Y_j$ can take arbitrarily large values.
    Thus this case is not allowed.
\end{itemize}
Thus for any direction of recession $\eta$ we must have all coordinates
nonpositive, and we must have $\eta_j < 0$ implies $y_j = 0$.

We can search for such vectors with linear programming.  Consider the
following linear programming problem.  Let $I = \{1, \ldots, k\}$ be the
index set of the vectors $y$ and $\eta$ and the row index set of the model
matrix $M$, and $J = \{1, \ldots, p\}$ be the
index set of the vector $\delta$ and the column index set of the model
matrix $M$, and let $m_{i j}$ denote the components of the model matrix.
\begin{alignat}{2}
  \text{minimize}   & \ \sum_{\substack{i \in I \\ y_i = 0}} \sum_{j \in J}
      m_{i j} \delta_j
  \nonumber
  \\
  \text{subject to} & \ \sum_{j \in J} m_{i j} \delta_j = 0,
      & \qquad & i \in I \opand y_i > 0
  \label{prog:no-mu-hat}
  \\
                    & \ -1 \le \sum_{j \in J} m_{i j} \delta_j \le 0,
      & & i \in I \opand y_i = 0
  \nonumber
\end{alignat}

\begin{theorem}
An MLE exists in the conventional sense (for the canonical parameter)
if and only if the linear program \eqref{prog:no-mu-hat} has optimal value
zero.  When the optimal value is negative, the solution $\delta$ is a
direction of recession that is not a direction of constancy and taking
limits in that direction gives a limiting conditional model having smaller
support than the original model.
\end{theorem}
See Theorem~6 and the following discussion in \citet{gdor} for discussion
of limits in directions of recession and the resulting limiting conditional
models.
\begin{proof}
As the discussion preceding the theorem says, we have a DOR
if and only if $\eta_j \le 0$ for all $j$ and $\eta_j < 0$ implies $y_j = 0$.
Redoing that discussion with DOC instead of DOR, we have a DOC
if and only if $\eta_j = 0$ for all $j$, that is, if $\delta$ is in the
null space of the model matrix.
By Theorem~{4} in \citet{gdor} the MLE exists in the conventional sense
if and only if every DOR is a DOC.  But since the optimal value of the
linear program is the sum of the components of $\eta$, this optimal value
is zero if and only if there is no $\delta$ such that any component of
$\eta$ is nonzero, that is if there is no DOR that is not also a DOC.

Conversely, if the optimal value is nonzero, then there is a delta that
is a DOR and the rest follows from Theorem~{6} in \citet{gdor} and the
discussion following that theorem in that article.
\end{proof}

Although this linear program is guaranteed to find a DOR that is not a DOC
if one exists, it is not guaranteed to find a \emph{generic} DOR \citep{gdor}.
Thus this algorithm may need to be iterated.
We can change the objective function (only) of the linear program
to seek other DOR that make other components of $\mu$ equal to zero that
we have not yet found.

\section{Practice}

\subsection{R Packages}

<<packages>>=
library(clpAPI)
library(Matrix)
@

\subsection{Data}

We do an example taking data from a draft paper \citep{eck-geyer}
that is too big to do using the methods of \citet{gdor} in a reasonable
amount of time (they take several days of computing time).
<<read.big.data>>=
foo <- paste0("https://conservancy.umn.edu/bitstream/handle/",
    "11299/197369/bigcategorical.txt")
bar <- sub("^.*/", "", foo)
if (! file.exists(bar)) download.file(foo, bar)
bigcategorical <- read.table(bar, header = TRUE,
    stringsAsFactors = TRUE)
class(bigcategorical)
dim(bigcategorical)
names(bigcategorical)
@

\subsection{Model Matrix}

We need the model matrix to construct the linear program.
In order to not blow out memory, we use R function \code{sparse.model.matrix}
from R package \code{Matrix}.
<<sparse.model.matrix.bigcategorical>>=
m <- sparse.model.matrix(y ~ 0 + (.)^4, data = bigcategorical)
dim(m)
@

\subsection{Linear Program Specification}

\subsubsection{Zeros of Response Vector}

<<zeros.bigcategorical>>=
is.zero <- bigcategorical$y == 0
@

\subsubsection{Objective Function}

Objective function gradient
<<objgrd.bigcategorical>>=
objgrd <- rbind(as.numeric(is.zero)) %*% m
objgrd <- as(objgrd, "numeric")
@

\subsubsection{Initialize LP Object}

<<clpAPI.initialize.bigcategorical>>=
lp <- initProbCLP()
@

Set row number
<<clpAPI.resize.bigcategorical>>=
resizeCLP(lp, nrow(m), 0)
@

And also this, to shut up printout to standard error.
<<clpAPI.blather.bigcategorical>>=
setLogLevelCLP(lp, 0)
@

\subsubsection{Put in LP Data}

Add columns
<<clpAPI.add.columns.bigcategorical>>=
foo <- rep(Inf, ncol(m))
addColsCLP(lp, ncol(m), -foo, foo, objgrd, m@p, m@i, m@x)
@
Set the row bounds
<<clpAPI.row.bounds.bigcategorical>>=
chgRowLowerCLP(lp, -as.numeric(is.zero))
chgRowUpperCLP(lp, rep(0, nrow(m)))
@
Set optimization "direction" to minimize.
<<clpAPI.minimize.bigcategorical>>=
setObjDirCLP(lp, 1)
@

\subsection{Solve LP}

R package \code{clpAPI} has five functions to solve linear programs.
Not knowing which one to use, we try one.
<<clpAPI.solve.bigcategorical>>=
lptime <- system.time(primalCLP(lp))
getSolStatusCLP(lp)
@
Solution status = 0 means optimal.
<<solution.time>>=
lptime
@

The solution is a direction of recession (DOR)
<<clpAPI.delta.bigcategorical>>=
delta <- getColPrimCLP(lp)
@

\subsection{Optimal Value}

<<clpAPI.optimal.value.bigcategorical>>=
getObjValCLP(lp)
@
This says that we have found a DOR that is not a DOC.
So MLE do not exist in the conventional sense and we have
to find the corresponding LCM.

\subsection{Check Solution}

Check that we have solution.  First map DOR to saturated model
canonical parameter scale.
<<eta.bigcategorical>>=
eta <- m %*% delta
eta <- as(eta, "numeric")
@

Now check all components of $\eta$ nonnegative
<<eta.check.one.bigcategorical>>=
max(eta)
@
Inaccuracy of computer arithmetic strikes again.  I guess that's OK.

Now check that $\eta_i < 0$ implies $y_i = 0$.
Actually check the contrapositive: $y_i > 0$ implies $\eta_i = 0$.
<<eta.check.two.bigcategorical>>=
range(eta[bigcategorical$y > 0])
@
Looks OK, but tolerance is very sloppy.  What is next lower?
<<eta.check.three.bigcategorical>>=
foo <- min(eta[bigcategorical$y > 0])
range(eta[eta < foo])
@

\subsection{Linearity}

Now we check what \citet{gdor} calls the linearity following R package
\code{rcdd}.  What is the dimension of the linearity?

First we get some idea of sizes of components of $\eta$.
<<eta.size.bigcategorical>>=
eta.sorted <- sort(eta)
eta.sorted.diff <- diff(eta.sorted)
i <- which(eta.sorted.diff == max(eta.sorted.diff))
eta.sorted[seq(i - 2, i + 2)]
@
So it looks like only very large negative values of components of $\eta$,
nearly equal to $-1$ are to be considered nonzero.
And these indicate components of the response vector constrained to be
zero in the LCM.
<<is.zero.lcm.bigcategorical>>=
is.zero.lcm <- eta < -1e-3
sum(is.zero.lcm)
sum(!is.zero.lcm)
sum(is.zero & (! is.zero.lcm))
@

This disagrees with the analysis in the supplementary material of
the paper where these data come from \citep{eck-geyer}.
They say 82 components of the response vector are constrained to be
zero in the LCM.  We only have, in our analysis so far,
\Sexpr{sum(is.zero.lcm)}.
So we iterate.

\section{Practice: Second Iteration}

\subsection{Save DOR}

Save the DOR already found
<<save.gdor>>=
save.delta <- delta
@

\subsection{Modify Linear Program}

<<iter.two.modify.objfun>>=
is.zero.unknown <- is.zero & (! is.zero.lcm)
objgrd <- rbind(as.numeric(is.zero.unknown)) %*% m
objgrd <- as(objgrd, "numeric")
chgObjCoefsCLP(lp, objgrd)
@

\subsection{Solve LP}

<<iter.two.solve>>=
lptime <- system.time(primalCLP(lp))
getSolStatusCLP(lp) == 0
lptime
delta <- getColPrimCLP(lp)
@

\subsection{Optimal Value}

<<iter.two.optimal.value>>=
getObjValCLP(lp)
@
This says that we have found a DOR that is not a DOC.
So MLE do not exist in the conventional sense in the LCM we have
found so far (which constrains \Sexpr{sum(is.zero.lcm)} components
of the response vector to be equal to zero) and we have
to find an LCM with smaller support (that constrains even more
components of the response vector to be equal to zero).

\subsection{Check Solution}

Check that we have solution.  First map DOR to saturated model
canonical parameter scale.
<<iter.two.eta>>=
eta <- m %*% delta
eta <- as(eta, "numeric")
@

Now check
<<iter.two.eta.check>>=
# all components of eta nonnegative
max(eta)
# y[j] > 0 implies eta[j] == 0
range(eta[bigcategorical$y > 0])
@
Looks OK.

\subsection{Linearity}

First we get some idea of sizes of components of $\eta$.
<<iter.two.eta.size>>=
eta.sorted <- sort(eta)
eta.sorted.diff <- diff(eta.sorted)
i <- which(eta.sorted.diff == max(eta.sorted.diff))
eta.sorted[seq(i - 2, i + 2)]
@
Now there is an even clearer gap between apparently zero and apparently nonzero.

So it looks like only very large negative values of components of $\eta$,
nearly equal to $-1$ are to be considered nonzero.
And these indicate components of the response vector constrained to be
zero in the LCM.
<<iter.two.is.zero.lcm>>=
is.zero.lcm <- is.zero.lcm | eta < -1e-3
sum(is.zero.lcm)
sum(!is.zero.lcm)
sum(is.zero & (! is.zero.lcm))
@

This disagrees with the analysis in the supplementary material of
the paper where these data come from \citep{eck-geyer}.
They say 82 components of the response vector are constrained to be
zero in the LCM.  We only have, in our analysis so far,
\Sexpr{sum(is.zero.lcm)}.
So we iterate again.

\section{Practice: Third Iteration}

\subsection{Save DOR}

Save the DOR already found
<<iter.three.save.gdor>>=
save.delta <- cbind(save.delta, delta)
@

\pagebreak[3]
\subsection{Modify Linear Program}

<<iter.three.modify.objfun>>=
is.zero.unknown <- is.zero & (! is.zero.lcm)
objgrd <- rbind(as.numeric(is.zero.unknown)) %*% m
objgrd <- as(objgrd, "numeric")
chgObjCoefsCLP(lp, objgrd)
@

\subsection{Solve LP}

<<iter.three.solve>>=
lptime <- system.time(primalCLP(lp))
getSolStatusCLP(lp) == 0
lptime
delta <- getColPrimCLP(lp)
@

\subsection{Optimal Value}

<<iter.three.optimal.value>>=
getObjValCLP(lp)
@
This says that we have found a DOR that is not a DOC.
So MLE do not exist in the conventional sense in the LCM we have
found so far (which constrains \Sexpr{sum(is.zero.lcm)} components
of the response vector to be equal to zero) and we have
to find an LCM with smaller support (that constrains even more
components of the response vector to be equal to zero).

\subsection{Check Solution}

Check that we have solution.  First map DOR to saturated model
canonical parameter scale.
<<iter.three.eta>>=
eta <- m %*% delta
eta <- as(eta, "numeric")
@

Now check
<<iter.three.eta.check>>=
# all components of eta nonnegative
max(eta)
# y[j] > 0 implies eta[j] == 0
range(eta[bigcategorical$y > 0])
@
Looks OK.

\subsection{Linearity}

First we get some idea of sizes of components of $\eta$.
<<iter.three.eta.size>>=
eta.sorted <- sort(eta)
eta.sorted.diff <- diff(eta.sorted)
i <- which(eta.sorted.diff == max(eta.sorted.diff))
eta.sorted[seq(i - 2, i + 2)]
@
Now there is an even clearer gap between apparently zero and apparently nonzero.

So it looks like only very large negative values of components of $\eta$,
nearly equal to $-1$ are to be considered nonzero.
And these indicate components of the response vector constrained to be
zero in the LCM.
<<iter.three.is.zero.lcm>>=
is.zero.lcm <- is.zero.lcm | eta < -1e-3
sum(is.zero.lcm)
sum(!is.zero.lcm)
sum(is.zero & (! is.zero.lcm))
@

This now agrees with the analysis in the supplementary material of
the paper where these data come from \citep{eck-geyer}.
They say 82 components of the response vector are constrained to be
zero in the LCM.  We now also have \Sexpr{sum(is.zero.lcm)}.

But we still need to do one more iteration to prove that we have
an LCM with the smallest possible support.

\section{Practice: Fourth Iteration}

\subsection{Save DOR}

Save the DOR already found
<<iter.four.save.gdor>>=
save.delta <- cbind(save.delta, delta)
@

\subsection{Modify Linear Program}

<<iter.four.modify.objfun>>=
is.zero.unknown <- is.zero & (! is.zero.lcm)
objgrd <- rbind(as.numeric(is.zero.unknown)) %*% m
objgrd <- as(objgrd, "numeric")
chgObjCoefsCLP(lp, objgrd)
@

\subsection{Solve LP}

<<iter.four.solve>>=
lptime <- system.time(primalCLP(lp))
getSolStatusCLP(lp) == 0
lptime
delta <- getColPrimCLP(lp)
@

\subsection{Optimal Value}

<<iter.four.optimal.value>>=
getObjValCLP(lp)
@
This says that every DOR of the LCM (that we have found so far,
which constrains \Sexpr{sum(is.zero.lcm)} components
of the response vector to be equal to zero) is also a DOC.
So MLE do exist in the conventional sense in this LCM
And this terminates our iteration.
We have solved what \citet{thesis} calls the
``Phase~I'' maximum likelihood problem.

\section{Which Components are Zero in LCM?}

<<check.zeros>>=
which(is.zero.lcm)
@
These seem to agree with the analysis in \citet{eck-geyer}.

\section{Generic Direction of Recession}

If we want a \emph{generic} DOR (GDOR) \citep{gdor},
then we have one: any positive combination of the DOR already found.
<<gdor>>=
gdor <- rowSums(save.delta)
@

Let us check that this works.
<<gdor.check>>=
gdor.eta <- m %*% gdor
gdor.eta <- as(gdor.eta, "numeric")
# all components of eta nonnegative
max(gdor.eta)
# y[j] > 0 implies eta[j] == 0
range(gdor.eta[bigcategorical$y > 0])
# gaps in size
max(gdor.eta[gdor.eta < -1e-3])
min(gdor.eta[gdor.eta > -1e-3])
@

And check the linearity it determines is same as we already had.
<<gdor.linearity>>=
identical(is.zero.lcm, gdor.eta < -1e-3)
@

\section{Fit LCM}

<<fit.lcm,error=TRUE>>=
gout <- glm(y ~ 0 + (.)^4, family = poisson,
    data = bigcategorical, subset = ! is.zero.lcm)
sum(is.na(coef(gout)))
range(predict(gout, type = "response"))
@

\section{Redo}

In this section we redo the calculation using a function to show what
is the minimal amount of code needed to do the job.
<<function>>=
fred <- function(modmat, response, tolerance = 1e-3) {
    stopifnot(inherits(modmat, "Matrix"))
    stopifnot(is.numeric(modmat@x))
    stopifnot(is.finite(modmat@x))
    stopifnot(is.numeric(response))
    stopifnot(is.finite(response))
    stopifnot(length(response) == nrow(modmat))
    stopifnot(round(response) == response)
    stopifnot(response >= 0)

    is.zero <- response == 0

    objgrd <- rbind(as.numeric(is.zero)) %*% modmat
    objgrd <- as(objgrd, "numeric")

    lp <- initProbCLP()
    resizeCLP(lp, nrow(modmat), 0)
    setLogLevelCLP(lp, 0)
    foo <- rep(Inf, ncol(modmat))
    addColsCLP(lp, ncol(modmat), -foo, foo, objgrd,
        modmat@p, modmat@i, modmat@x)
    chgRowLowerCLP(lp, - as.numeric(is.zero))
    chgRowUpperCLP(lp, rep(0, nrow(modmat)))
    setObjDirCLP(lp, 1)

    save.delta <- NULL
    is.zero.lcm <- rep(FALSE, length(response))
    repeat {
        primalCLP(lp)
        stopifnot(getSolStatusCLP(lp) == 0)
        if (getObjValCLP(lp) > (- tolerance)) break
        delta <- getColPrimCLP(lp)
        save.delta <- cbind(save.delta, delta)
        eta <- modmat %*% delta
        eta <- as(eta, "numeric")
        is.zero.lcm <- is.zero.lcm | eta < (- tolerance)
        is.zero.unknown <- is.zero & (! is.zero.lcm)
        objgrd <- rbind(as.numeric(is.zero.unknown)) %*% modmat
        objgrd <- as(objgrd, "numeric")
        chgObjCoefsCLP(lp, objgrd)
    }
    if (is.null(save.delta)) return(list(is.lcm = FALSE,
        is.zero.lcm = is.zero.lcm, delta = save.delta,
        gdor = NULL)) else return(list(is.lcm = TRUE,
        is.zero.lcm = is.zero.lcm, delta = save.delta,
        gdor = rowSums(save.delta)))
}
@
Try it.
<<try>>=
time.fred <- system.time(fout <- fred(m, bigcategorical$y))
time.fred
fout$is.lcm
identical(is.zero.lcm, fout$is.zero.lcm)
all.equal(gdor, fout$gdor)
@

\begin{thebibliography}{}

\bibitem[Eck and Geyer(2018)]{eck-geyer}
Eck, D.~J., and Geyer, C.~J. (2018).
\newblock Computationally efficient likelihood inference in exponential
    families when the maximum likelihood estimator does not exist.
\newblock \url{https://arxiv.org/abs/1803.11240}.

\bibitem[Geyer(1990)]{thesis}
Geyer, C.~J. (1990).
\newblock Likelihood and Exponential Families.
\newblock PhD thesis, University of Washington.
\newblock \url{http://hdl.handle.net/11299/56330}.

\bibitem[Geyer(2009)]{gdor}
Geyer, C.~J. (2009).
\newblock Likelihood inference in exponential families and directions of
    recession.
\newblock \emph{Electronic Journal of Statistics}, \textbf{3}, 259--289
    (electronic).
\newblock \url{http:www.stat.umn.edu/geyer/gdor/}.

\bibitem[Geyer(2016a)]{expfam}
Geyer, C.~J. (2016a).
\newblock Stat 5421 Lecture Notes: Exponential Families, Part I.
\newblock \url{http://www.stat.umn.edu/geyer/5421/notes/expfam.pdf}.

\bibitem[Geyer(2016b)]{infinity}
Geyer, C.~J. (2016b).
\newblock Stat 5421 Lecture Notes: Exponential Families, Part II.
\newblock \url{http://www.stat.umn.edu/geyer/5421/notes/infinity.pdf}.

\end{thebibliography}

\end{document}

