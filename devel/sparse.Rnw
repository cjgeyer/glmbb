
\documentclass[11pt]{article}

\usepackage{amsmath}
\usepackage{indentfirst}
\usepackage{natbib}
\usepackage{url}

\let\code=\texttt

\newcommand{\opand}{\mathbin{\rm and}}

\newcommand{\REVISED}{\begin{center} \LARGE REVISED DOWN TO HERE \end{center}}
\newcommand{\MOVED}[1][equation]{\begin{center} [#1 moved] \end{center}}

%\VignetteEngine{knitr::knitr}

\begin{document}

\title{Generalized Directions of Recession for Generalized Linear Models}

\author{Charles J. Geyer}

\maketitle

<<options,include=FALSE,echo=FALSE>>=
options(keep.source = TRUE, width = 60)
@

\section{Data}

We will use the data for the example of Section~{2.3} in \citet{gdor}.
<<data>>=
u <- "http://www.stat.umn.edu/geyer/gdor/catrec.txt"
dat <- read.table(u, header = TRUE)
@

\section{Fit Model}

<<fit, error=TRUE>>=
gout <- glm(y ~ (v1 + v2 + v3 + v4 + v5 + v6 + v7)^3,
    data = dat, family = "poisson")
@

\section{Sparse Model Matrix}

<<sparse.model.matrix>>=
library("Matrix")
m <- sparse.model.matrix(
    y ~ (v1 + v2 + v3 + v4 + v5 + v6 + v7)^3,
    data = dat)
@

\section{Directions of Recession}

OK.  Now we want to solve for a generic direction of recession.
For the example we are doing, we only need to know how to handle Poisson
response.

From Section~3.9 of \citet{gdor} we see that if $y$ is the response vector
and $M$ is the model matrix, then $M^T y$ is the submodel canonical sufficient
statistic vector.  Theorems 1, 3, and {4} in \citet{gdor} say that the
maximum likelihood estimate (MLE) exists for the submodel canonical parameter
vector (called the coefficients vector by R function \code{glm})
if and only if every direction of recession of the log likelihood is also
a direction of constancy, where if $Y$ denotes a random realization of the
response vector and $y$ the observed value of the response vector, a vector
$\delta$ in the submodel canonical parameter space is
\begin{itemize}
\item a \emph{direction of recession} if $(Y - y)^T M \delta \le 0$
    almost surely and
\item and a \emph{direction of constancy} if $(Y - y)^T M \delta = 0$
    almost surely.
\end{itemize}

Suppose $\delta$ is a nonzero direction of recession
and define $\eta = M \delta$.  Then by definition of direction of recession,
we have $Y^T \eta \le y^T \eta$ almost surely.
\begin{itemize}
\item If $\eta_j = 0$, this says nothing about $Y_j$.
\item If $\eta_j < 0$, this says $Y_j \ge y_j$ almost surely because it
    is possible that all coordinates of $Y$ are zero except for $Y_j$.
    And this implies $y_j = 0$ because it it possible for $Y_j$ to be zero.
\item If $\eta_j > 0$, this says $Y_j \le y_j$ almost surely, and this is
    impossible because $Y_j$ can take arbitrarily large values.  Thus this
    case in not allowed.
\end{itemize}
Thus for any direction of recession $\eta$ we must have all coordinates
nonpositive, and we must have $\eta_j < 0$ implies $y_j = 0$.

We can search for such vectors with linear programming.  Consider the
following linear programming problem.  Let $I = \{1, \ldots, k\}$ be the
index set of the vectors $y$ and $\eta$ and the row index set of the model
matrix $M$, and $J = \{1, \ldots, p\}$ be the
index set of the vector $\delta$ and the column index set of the model
matrix $M$, and let $m_{i j}$ denote the components of the model matrix.
\begin{alignat}{2}
  \text{minimize}   & \ \sum_{\substack{i \in I \\ y_i = 0}} \sum_{j \in J}
      m_{i j} \delta_j
  \nonumber
  \\
  \text{subject to} & \ \sum_{j \in J} m_{i j} \delta_j = 0,
      & \qquad & i \in I \opand y_i > 0
  \label{prog:no-mu-hat}
  \\
                    & \ -1 \le \sum_{j \in J} m_{i j} \delta_j \le 0,
      & & i \in I \opand y_i = 0
  \nonumber
\end{alignat}
In fact we can do better than this if we have a fitted data vector $\hat{\mu}$
from an attempt by R function \code{glm} (or some other likelihood maximizer)
to maximize the likelihood.  From Theorem~{6} in \citet{gdor} we must have
$(Y - y)^T M \delta = 0$ almost surely for the limiting conditional model.
Hence we must have $(\mu - y)^T M \delta = 0$ almost surely.  Hence
(still using $\eta = M \delta$) we must have $\eta_j < 0$ implies $\mu_j = 0$
and, conversely, $\mu_j > 0$ implies $\eta_j = 0$.

Due to inexactness of computer arithmetic and due to the approximateness
of computer optimization routines, we can never tell whether what the computer
reports as fitted values are zero or not.  We have to use a tolerance.
To respect this issue we adopt the notation $\hat{\mu}_j \gg 0$ indicates
that $\hat{\mu}_j$ is larger than the tolerance and $\hat{\mu}_j \simeq 0$
indicates that $\hat{\mu}_j$ is not larger than the tolerance.
Then we can use this linear program
\begin{alignat}{2}
  \text{minimize}   & \ \sum_{\substack{i \in I \\ \hat{\mu}_i \simeq 0}}
      \sum_{j \in J} m_{i j} \delta_j
  \nonumber
  \\
  \text{subject to} & \ \sum_{j \in J} m_{i j} \delta_j = 0,
      & \qquad & i \in I \opand \hat{\mu}_i \gg 0
  \label{prog:mu-hat}
  \\
                    & \ -1 \le \sum_{j \in J} m_{i j} \delta_j \le 0,
      & & i \in I \opand \hat{\mu}_i \simeq 0
  \nonumber
\end{alignat}

The fact that our tolerance may be wrong and we may be misclassifying
some of the components of $\hat{\mu}$ as zero or nonzero does no harm
so long as the tolerance is large enough.  If we decide $\hat{\mu}_j \simeq 0$
when in fact $\mu_j > 0$, then our linear program just has to do more work
but is not wrong.  Any solution is still a direction of recession.
In fact, no matter what, we can still easily check whether a solution of
our linear program is a direction of recession (just check that $\eta_j \le 0$
for all $j$ and $\eta_j < 0$ implies $y_j = 0$).

Note that with using $\hat{\mu}$ to determine the linear programming problem,
there is often nothing to do.  If we have $\hat{\mu}_i \gg 0$ for all $i$ then
the only directions of recession are vectors in the null space of $M$ and
those are also directions of constancy, so the MLE exists in the classical
sense.

It is only when $\hat{\mu}_j \simeq 0$ for some $j$ (and typically R function
\code{glm} issues a warning) that we have to follow up with doing a linear
program.

\section{Items for the Linear Program}

Now we need to figure out a few things about the data and the fitted model.
<<mu-hat>>=
mu.hat <- gout$fitted
min(mu.hat)
max(mu.hat[mu.hat < 1000 * min(mu.hat)])
min(mu.hat[mu.hat > 1000 * min(mu.hat)])
@
For these data it is clear what the zeros are (we think).  There are
roughly eight orders of magnitude between the apparent zeros and the
apparent nonzeros.
<<mu-hat-is-zero>>=
is.zero <- mu.hat < 1000 * min(mu.hat)
sum(is.zero)
sum(gout$y[is.zero])
@

Now the gradient of the objective function is
<<objgrd>>=
objgrd <- rbind(as.numeric(is.zero)) %*% m
objgrd <- as(objgrd, "numeric")
@

\section{Use COIN-OR}

We would like to be able to do really large scale problems with no worries.
R package \code{clpAPI} purports to do this.  Let's try it out.
<<clpAPI-library>>=
library("clpAPI")
@

The first thing is to make a model object.
<<clpAPI-initialize>>=
lp <- initProbCLP()
@

Apparently, we need the following.
<<clpAPI-resize>>=
resizeCLP(lp, nrow(m), 0)
@

And also this, to shut up printout to standard error.
<<clpAPI-blather>>=
setLogLevelCLP(lp, 0)
@

Now we put a bunch of stuff into the linear program, mainly the constraint
matrix, but also bounds on the variables (there are none but we have to say
that).
<<clpAPI-add-columns>>=
foo <- rep(Inf, ncol(m))
addColsCLP(lp, ncol(m), -foo, foo, objgrd, m@p, m@i, m@x)
@

Now we have to set the row bounds
<<clpAPI-row-bounds>>=
chgRowLowerCLP(lp, -as.numeric(is.zero))
chgRowUpperCLP(lp, rep(0, nrow(m)))
@

And the optimization "direction" to minimize.
<<clpAPI-minimize>>=
setObjDirCLP(lp, 1)
@

\section{Solve}

And we think we are ready to solve the linear programming problem.
<<clpAPI-solve>>=
primalCLP(lp)
getSolStatusCLP(lp)
@
Solution status = 0 means optimal.
<<clpAPI-delta>>=
delta <- getColPrimCLP(lp)
@
The solution is a generalized direction of recession (GDOR).

<<print-dor>>=
names(delta) <- names(gout$coefficients)
cbind(delta)[zapsmall(delta) != 0, , drop = FALSE]
@
Completely agrees with Table~{1} in \citet{gdor}.

\section{Destruct}

And the last thing we do is destruct the model object.
<<clpAPI-dispose>>=
delProbCLP(lp)
@

\appendix

We do a bigger example
<<read-big-data>>=
foo <- "https://conservancy.umn.edu/bitstream/handle/11299/197369/bigcategorical.txt"
bar <- sub("^.*/", "", foo)
if (! file.exists(bar))
download.file(foo, bar)
bigcategorical <- read.table(bar, header = TRUE, stringsAsFactors = TRUE)
class(bigcategorical)
dim(bigcategorical)
names(bigcategorical)
@
Fit model specified in the paper
(all 4 way and no 5 way interactions).
<<big-data-fit,error=TRUE,cache=TRUE>>=
library(biglm)
gout <- bigglm(y ~ 0 + (x1 + x2 + x3 + x4 + x5)^4,
    family = poisson(), data = bigcategorical, maxit = 200)
gout$converged
cout <- coef(gout)
length(cout)
@

Now get mean-value parameters
<<big-data-mean-value,error=TRUE,cache=TRUE>>=
mu <- predict(gout, newdata = bigcategorical, type = "response")
length(mu)
foo <- log10(sort(mu))
dfoo <- diff(foo)
i <- which(dfoo == max(dfoo))
i
foo[seq(i - 2, i + 2)]
is.zero <- mu < 1e-9
max(mu[is.zero])
min(mu[! is.zero])

foo <- log10(sort(mu[! is.zero]))
dfoo <- diff(foo)
i <- which(dfoo == max(dfoo))
i
foo[seq(i, i + 4)]
@
Good enough we hope.  Now find direction of recession as above.
<<sparse.model.matrix.bigcategorical>>=
m <- sparse.model.matrix(
    y ~ 0 + (x1 + x2 + x3 + x4 + x5)^4,
    data = bigcategorical)
dim(m)
@
Objective function gradient
<<objgrd.bigcategorical>>=
objgrd <- rbind(as.numeric(is.zero)) %*% m
objgrd <- as(objgrd, "numeric")
@
Initialize CLP object
<<clpAPI-initialize-bigcategorical>>=
lp <- initProbCLP()
@
Set row number
<<clpAPI-resize.bigcategorical>>=
resizeCLP(lp, nrow(m), 0)
@
And also this, to shut up printout to standard error.
<<clpAPI-blather.bigcategorical>>=
setLogLevelCLP(lp, 0)
@
Add columns
<<clpAPI-add-columns.bigcategorical>>=
foo <- rep(Inf, ncol(m))
addColsCLP(lp, ncol(m), -foo, foo, objgrd, m@p, m@i, m@x)
@
Set the row bounds
<<clpAPI-row-bounds.bigcategorical>>=
chgRowLowerCLP(lp, -as.numeric(is.zero))
chgRowUpperCLP(lp, rep(0, nrow(m)))
@
Set optimization "direction" to minimize.
<<clpAPI-minimize.bigcategorical>>=
setObjDirCLP(lp, 1)
@
Solve
<<clpAPI-solve.bigcategorical>>=
lptime <- system.time(primalCLP(lp))
getSolStatusCLP(lp)
@
Solution status = 0 means optimal.
<<solution.time>>=
lptime
@

The solution is a generalized direction of recession (GDOR)
<<clpAPI-delta.bigcategorical>>=
delta <- getColPrimCLP(lp)
@

Check that we have solution.  First map GDOR to saturated model
canonical parameter scale.
<<eta.bigcategorical>>=
eta <- m %*% delta
eta <- as(eta, "numeric")
@

Now check all components of $\eta$ nonnegative
<<eta.check.one.bigcategorical>>=
max(eta)
@
Inaccuracy of computer arithmetic strikes again.  I guess that's OK.

Now check that $\eta_i < 0$ implies $y_i = 0$.
Actually check the contrapositive: $y_i > 0$ implies $\eta_i = 0$.
<<eta.check.two.bigcategorical>>=
range(eta[bigcategorical$y > 0])
range(eta[bigcategorical$y == 0])
@
Looks OK.

What is the dimension of the linearity?
<<linearity.bigcategorical>>=
sum(abs(eta) < 1e-9)
sum(abs(eta) >= 1e-9)
@
Does not look OK.  Looks like we have a DOR but not a GDOR.  This does
not agree with the supplementary material for the paper from which these
data were taken.  Try again.  Redefine \code{is.zero} to be just \code{y == 0}.
<<big.data.is.zero>>=
is.zero <- bigcategorical$y == 0
@
Now have to redo \code{objgrd}
<<big.data.objgrd.too>>=
objgrd <- rbind(as.numeric(is.zero)) %*% m
objgrd <- as(objgrd, "numeric")
chgObjCoefsCLP(lp, objgrd)
@
and row bounds of linear program
<<big.data.row.bounds.too>>=
chgRowLowerCLP(lp, -as.numeric(is.zero))
@

And re-solve
<<clpAPI-solve.bigcategorical.too,cache=TRUE>>=
lptime <- system.time(primalCLP(lp))
getSolStatusCLP(lp)
@
Show time
<<clpAPI-solve.bigcategorical.too.time>>=
lptime
@

Get GDOR
<<clpAPI-delta.bigcategorical.too>>=
delta.save <- delta
delta <- getColPrimCLP(lp)
max(abs(delta - delta.save))
@
Looks like we have something different.   Redo checks.
<<eta.bigcategorical.too>>=
eta <- m %*% delta
eta <- as(eta, "numeric")

# check all components nonnegative
max(eta)

# check y[i] > 0 implies eta[i] == 0
range(eta[bigcategorical$y > 0])
range(eta[bigcategorical$y == 0])
@
Looks OK.

What is the dimension of the linearity?
<<linearity.bigcategorical.too>>=
sum(abs(eta) < sqrt(.Machine$double.eps))
sum(abs(eta) >= sqrt(.Machine$double.eps))
@





\begin{thebibliography}{}

\bibitem[Geyer(2009)]{gdor}
Geyer, C.~J. (2009).
\newblock Likelihood inference in exponential families and directions of
    recession.
\newblock \emph{Electronic Journal of Statistics}, \textbf{3}, 259--289
    (electronic).

\end{thebibliography}

\end{document}

